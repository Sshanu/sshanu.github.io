<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <script type="text/javascript" src="js/hidebib.js"></script>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>Jon Barron</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr><td>

      <p align="center"><font size="7">Deepak Pathak</font><br>
          <b>Email</b>:
          <font id="email" style="display:inline;">dre@.cplaetshkeykba.ue <a href="#" onclick="emailScramble.initAnimatedBubbleSort();return false;">unscramble</a></font>
          <script>
          emailScramble = new scrambledString(document.getElementById('email'),
              'emailScramble', 'erepc@aahby.etulk.skde',
              [12,13,15,1,8,7,2,5,4,11,18,10,17,3,22,16,6,19,9,14,21,20]);
          </script>
        </p><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        
      
        <tbody><tr>
          <td width="60%" valign="middle" align="justify">
          <p>I am currently a PhD candidate in CS at UC Berkeley working on Artificial Intelligence at the intersection of Computer Vision, Machine Learning and Robotics. I am a part of <a href="https://bair.berkeley.edu/">Berkeley AI Research Lab (BAIR)</a>, advised by <a href="http://www.cs.berkeley.edu/~trevor/">Trevor Darrell</a> and <a href="http://www.cs.berkeley.edu/~efros/">Alyosha Efros</a>.</p>
      
          <p>My ultimate goal is to build agents with a human-like ability to <i>generalize</i> in real and diverse environments. I believe understanding how to <i>continually</i> develop knowledge and acquire new skills from just raw sensory data will play a vital role in achieving this goal. I draw inspiration from psychology to build practical systems at the interface of vision, learning and robotics that can learn using <i>data as its own supervision</i>. My research is supported by fellowships from <a href="https://research.fb.com/announcing-the-2018-cohort-of-facebook-fellows-and-emerging-scholars/">Facebook</a>, <a href="https://research.nvidia.com/grad_fellows/2017/DeepakPathak">Nvidia</a>, and <a href="https://snapresearchfellowship.splashthat.com/">Snapchat</a>.</p>
      
          <p>Earlier, I received a Bachelors in CS at IIT Kanpur, working with <a href="http://www.cse.iitk.ac.in/users/amit/">Amitabha Mukerjee</a>.<!-- My homepage from that time is archived <a href="iitk_website/index.html">here</a>.--> I have also spent time at <a href="https://www.ri.cmu.edu/">CMU RI</a>, <a href="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research</a> (Seattle, Pittsburgh), and <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a> (NYC).</p>
      
          <p align="center">
          <a href="CV_Pathak.pdf">CV</a> | <a href="https://scholar.google.cl/citations?user=AEsPCAUAAAAJ&amp;hl=en&amp;oi=ao">Google Scholar</a> | <a href="https://github.com/pathak22">Github</a> | <a href="https://twitter.com/pathak2206"> Twitter </a>
          </p>
          </td>
      
          <td width="40%" valign="top"><a href="images/Deepak_Pathak.jpg"><img src="images/Deepak_Pathak.jpg" width="100%"></a></td>
        </tr>
      </tbody></table>
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr><td>
          <sectionheading>News</sectionheading>
          <ul>
          <li> I am co-organizing the <a href="https://tarl2019.github.io/" target="_blank">Task Agnostic Reinforcement Learning</a> workshop at ICLR'19. Submission is open!</li>
          <li> <a href="#ICLR19">ICLR'19 paper</a> on large-scale curiosity-driven learning covered in <a href="https://pathak22.github.io/large-scale-curiosity/index.html#media">The Economist and other media articles</a>.</li>
          <li> I recently co-organized the <a href="http://pocv18.eecs.berkeley.edu/">Action, Perception and Organization</a> workshop at ECCV'18.</li>
          <li> Received the <a href="https://research.fb.com/announcing-the-2018-cohort-of-facebook-fellows-and-emerging-scholars/" target="_blank">2018 Facebook Graduate Fellowship</a>. Thanks, Facebook!</li>
          <li> <a href="#ICML18">ICML'18 paper</a> on human exploration got covered in <a href="https://rach0012.github.io/humanRL_website/index.html#media">MIT Tech Review and other media articles</a>.</li>
          <li> Received the <a href="https://snapresearchfellowship.splashthat.com/" target="_blank">2017 Snap Inc. Research Fellowship Award</a>. Thanks, Snapchat!</li>
          <li> <a href="#ICML17">ICML'17 paper</a> on curiosity-driven exploration got covered in <a href="https://pathak22.github.io/noreward-rl/index.html#media">MIT Tech Review and other media articles</a>.</li>
          <li> Received the <a href="http://research.nvidia.com/grad_fellows/2017/DeepakPathak" target="_blank">2017 NVIDIA Graduate Fellowship</a>. Thanks, NVIDIA! Complete list <a href="http://research.nvidia.com/grad_fellowship/2017" target="_blank">here</a>.</li>
          <!-- <li> <a href="#NIPS17">Paper</a> accepted at NIPS 2017 on multimodal image generation.</li> -->
          <!-- <li> <a href="#ICLR18">Paper</a> on zero-shot visual imitation accepted at <a href="https://openreview.net/group?id=ICLR.cc/2018/Conference" target="_blank">ICLR 2018</a>.</li> -->
          <!-- <li> <a href="#ICML17">Paper</a> accepted at ICML 2017 on curiosity-driven exploration for reinforcement learning. <a href="http://pathak22.github.io/noreward-rl/index.html#demoVideo">Video!</a></li> -->
          <!-- <li> <a href="#CVPR17">Paper</a> accepted at CVPR 2017 on unsupervised learning using unlabeled videos.</li> -->
          <!-- <li> <a href="#CVPR16">Paper</a> accepted at CVPR 2016 on unsupervised learning and inpainting. <a href="context_encoder/">Check out!</a></li> -->
          <!-- <li> <a href="#JMLR16">Paper</a> accepted at JMLR 2016; extension of <a href="#CVPR15">CVPR'15</a> paper.</li> -->
          <!-- <li> Paper about constrained structured regression (applied to intrinsics) on <a href="http://arxiv.org/abs/1511.07497">arXiv</a>.</li> -->
          <!-- <li> <a href="#ICCV15">Paper</a> accepted at ICCV 2015 on Constrained CNN for segmentation. Code released on <a href="http://github.com/pathak22/ccnn">github</a> !</li> -->
          <!-- <li> Undergrad <a href="#JPM15">paper</a> related to predicting <a href="http://www.huffingtonpost.co.uk/2015/02/23/microsoft-oscar-predictio_n_6735122.html">Oscars</a> published at <a href="http://ubplj.org/index.php/jpm/article/view/1048">JPM</a>. See <a href="#oscarPred">live predictions</a>.</li> -->
          <!-- <li> <a href="#CVPR15">Paper</a> accepted at CVPR 2015 on domain adaptation</li> -->
          <!-- <li> <a href="#ICLR15">Paper</a> accepted at ICLR Workshop 2015</li> -->
          </ul>
        </td></tr>
      </tbody></table>
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr><td><sectionheading>Publications</sectionheading></td></tr>
      </tbody></table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      
        <tbody><tr>
          <td width="33%" valign="top" align="center"><a href="https://pathak22.github.io/modular-assemblies/"><img src="images/assemblies19.gif" alt="sym" width="100%" style="border-style: none"></a>
          </td><td width="67%" valign="top">
            <p><a href="https://pathak22.github.io/modular-assemblies/" id="ASSEMBLIES19">
            <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
            <heading>Learning to Control Self-Assembling Morphologies:<br>A Study of Generalization via Modularity</heading></a><br>
            <strong>Deepak Pathak*</strong>, Chris Lu*, Trevor Darrell, Phillip Isola and Alexei A. Efros<br>
            <em>Pre-print on arXiv:1902.05546</em>, 2019 (under review)<br>
            </p>
      
            <div class="paper" id="assemblies19">
            <a href="https://pathak22.github.io/modular-assemblies/">project webpage</a> |
            <a href="https://pathak22.github.io/modular-assemblies/resources/assemblies.pdf">pdf</a> |
            <a href="javascript:toggleblock('assemblies19_abs')">abstract</a> |
            <a shape="rect" href="javascript:togglebib('assemblies19')" class="togglebib">bibtex</a> |
            <a href="https://arxiv.org/abs/1902.05546">arXiv</a> |
            <a href="https://youtu.be/cg-RdkPtRiQ">video</a> |
            <a href="https://pathak22.github.io/modular-assemblies/index.html#sourceCode">code</a>
            <br>
      
            <p align="justify"> <i id="assemblies19_abs" style="display: none;">Contemporary sensorimotor learning approaches typically start with an existing complex agent (e.g., a robotic arm), which they learn to control. In contrast, this paper investigates a modular co-evolution strategy: a collection of primitive agents learns to dynamically self-assemble into composite bodies while also learning to coordinate their behavior to control these bodies. Each primitive agent consists of a limb with a motor attached at one end. Limbs may choose to link up to form collectives. When a limb initiates a link-up action and there is another limb nearby, the latter is magnetically connected to the 'parent' limb's motor. This forms a new single agent, which may further link with other agents. In this way, complex morphologies can emerge, controlled by a policy whose architecture is in explicit correspondence with the morphology. We evaluate the performance of these <em>dynamic</em> and <em>modular</em> agents in simulated environments. We demonstrate better generalization to test-time changes both in the environment, as well as in the agent morphology, compared to static and monolithic baselines.</i></p>
      
      <pre xml:space="preserve" style="display: none;">@inproceedings{pathak19assemblies,
        Author = {Pathak, Deepak and
        Lu, Chris and Darrell, Trevor and
        Isola, Phillip and Efros, Alexei A.},
        Title = {Learning to Control Self-
        Assembling Morphologies: A Study of
        Generalization via Modularity},
        Booktitle = {arXiv preprint arXiv:1902.05546},
        Year = {2019}
      }
      </pre>
            </div>
          </td>
        </tr>
      
        <tr>
          <td width="33%" valign="top" align="center"><a href="https://pathak22.github.io/large-scale-curiosity/"><img src="images/iclr19.jpg" alt="sym" width="100%" style="border-style: none"></a>
          </td><td width="67%" valign="top">
            <p><a href="https://pathak22.github.io/large-scale-curiosity/" id="ICLR19">
            <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
            <heading>Large-Scale Study of Curiosity-Driven Learning</heading></a><br>
            Yuri Burda*, Harri Edwards*, <strong>Deepak Pathak*</strong>, Amos Storkey, Trevor Darrell and Alexei A. Efros &nbsp;&nbsp;&nbsp;&nbsp;(* equal contribution, alphabetical)<br>
            <em>International Conference on Learning Representations (ICLR)</em>, 2019<br>
            </p>
      
            <div class="paper" id="iclr19">
            <a href="https://pathak22.github.io/large-scale-curiosity/">project webpage</a> |
            <a href="https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf">pdf</a> |
            <a href="javascript:toggleblock('iclr19_abs')">abstract</a> |
            <a shape="rect" href="javascript:togglebib('iclr19')" class="togglebib">bibtex</a> |
            <a href="https://arxiv.org/abs/1808.04355">arXiv</a> |
            <a href="https://youtu.be/l1FqtAHfJLI">video</a>
            <!-- <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a> -->
            <br>
            Also presented at NIPS'18 Deep RL Workshop (Oral)<br>
      
            <p align="justify"> <i id="iclr19_abs" style="display: none;">Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups.</i></p>
      
      <pre xml:space="preserve" style="display: none;">@inproceedings{pathakICLR19largescale,
        Author = {Burda, Yuri and
        Edwards, Harri and Pathak, Deepak and
        Storkey, Amos and Darrell, Trevor and
        Efros, Alexei A.},
        Title = {Large-Scale Study of
        Curiosity-Driven Learning},
        Booktitle = {ICLR},
        Year = {2019}
      }
      </pre>
            </div>
          </td>
        </tr>
      
        <tr>
          <td width="33%" valign="top" align="center"><a href="https://pathak22.github.io/seg-by-interaction/"><img src="images/cvprw18.jpg" alt="sym" width="100%" style="border-style: none"></a>
          </td><td width="67%" valign="top">
            <p><a href="https://pathak22.github.io/seg-by-interaction/" id="CVPRW18">
            <heading>Learning Instance Segmentation by Interaction</heading></a><br>
            <strong>Deepak Pathak*</strong>, Yide Shentu*, Dian Chen*, Pulkit Agrawal*, Trevor Darrell, Sergey Levine and Jitendra Malik<br>
            <!-- <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018<br> -->
            <em><a href="https://sites.google.com/view/cvpr2018-robotic-vision/home" style="color:black" target="_blank">Deep Learning in Robotics Vision Workshop</a> (CVPR)</em>, 2018 (Oral)<br>
            </p>
      
            <div class="paper" id="cvprw18">
            <a href="https://pathak22.github.io/seg-by-interaction/">project webpage</a> |
            <a href="papers/cvprw18.pdf">pdf</a> |
            <a href="javascript:toggleblock('cvprw18_abs')">abstract</a> |
            <a shape="rect" href="javascript:togglebib('cvprw18')" class="togglebib">bibtex</a> |
            <a href="https://arxiv.org/abs/1806.08354">arXiv</a> |
            <a href="https://pathak22.github.io/seg-by-interaction/index.html#sourceCode">code</a>
            <br>
      
            <p align="justify"> <i id="cvprw18_abs" style="display: none;">We present an approach for building an active agent that learns to segment its visual observations into individual objects by interacting with its environment in a completely self-supervised manner. The agent uses its current segmentation model to infer pixels that constitute objects and refines the segmentation model by interacting with these pixels. The model learned from over 50K interactions generalizes to novel objects and backgrounds. To deal with noisy training signal for segmenting objects obtained by self-supervised interactions, we propose robust set loss. A dataset of robot's interactions along-with a few human labeled examples is provided as a benchmark for future research. We test the utility of the learned segmentation model by providing results on a downstream vision-based control task of rearranging multiple objects into target configurations from visual inputs alone.</i></p>
      
      <pre xml:space="preserve" style="display: none;">@inproceedings{pathakCVPRW18segByInt,
            Author = {Pathak, Deepak and
            Shentu, Yide and Chen, Dian and
            Agrawal, Pulkit and Darrell, Trevor and
            Levine, Sergey and Malik, Jitendra},
            Title = {Learning Instance Segmentation
              by Interaction},
            Booktitle = {CVPR Workshop on Benchmarks for
              Deep Learning in Robotic Vision},
            Year = {2018}
        }
      </pre>
            </div>
          </td>
        </tr>
      
        <tr>
          <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1807.07560"><img src="images/compgan18.jpg" alt="sym" width="100%" style="border-style: none"></a>
          </td><td width="67%" valign="top">
            <p><a href="https://arxiv.org/abs/1807.07560" id="CompGAN18">
            <heading>Compositional GAN: Learning Conditional Image Composition</heading></a><br>
            Samaneh Azadi, <strong>Deepak Pathak</strong>, Sayna Ebrahimi and Trevor Darrell<br>
            Pre-print on arXiv:1807.07560, 2018
            <!-- <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018<br> -->
            </p>
      
            <div class="paper" id="compgan18">
            <a href="https://arxiv.org/abs/1807.07560">arXiv pdf</a> |
            <a href="javascript:toggleblock('compgan18_abs')">abstract</a> |
            <a shape="rect" href="javascript:togglebib('compgan18')" class="togglebib">bibtex</a>
            <!-- <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a> -->
            <br>
      
            <p align="justify"> <i id="compgan18_abs" style="display: none;">Generative Adversarial Networks (GANs) can produce images of surprising complexity and realism, but are generally modeled to sample from a single latent source ignoring the explicit spatial interaction between multiple entities that could be present in a scene. Capturing such complex interactions between different objects in the world, including their relative scaling, spatial layout, occlusion, or viewpoint transformation is a challenging problem. In this work, we propose to model object composition in a GAN framework as a self-consistent composition-decomposition network. Our model is conditioned on the object images from their marginal distributions to generate a realistic image from their joint distribution by explicitly learning the possible interactions. We evaluate our model through qualitative experiments and user evaluations in both the scenarios when either paired or unpaired examples for the individual object images and the joint scenes are given during training. Our results reveal that the learned model captures potential interactions between the two object domains given as input to output new instances of composed scene at test time in a reasonable fashion.</i></p>
      
      <pre xml:space="preserve" style="display: none;">@inproceedings{azadi18compgan,
        Author = {Azadi, Samaneh and
        Pathak, Deepak and
        Ebrahimi, Sayna and Darrell, Trevor},
        Title = {Compositional GAN: Learning
        Conditional Image Composition},
        Booktitle = {arXiv:1807.07560},
        Year = {2018}
      }
      </pre>
            </div>
          </td>
        </tr>
      
              
      </tbody></table>
      
      
<!--       
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody><tr><td>
          <sectionheading>Selected Awards</sectionheading>
          <ul>
          <li> Facebook Graduate Fellowship (2018-2020)</li>
          <li> Nvidia Graduate Fellowship (2017-2018)</li>
          <li> Snapchat Inc. Graduate Fellowship (2017)</li>
          <li> Gold Medal in Computer Science at IIT Kanpur (2014)</li>
          <li> Best Undergraduate Thesis Award at IIT Kanpur (2014)</li>
          </ul>
        </td></tr>
      </tbody></table> -->
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr><td><br><p align="right"><font size="2">
          Template: <a href="http://www.cs.berkeley.edu/~barron/">this</a>, <a href="http://www.cs.berkeley.edu/~sgupta/">this</a> and <a href="http://jeffdonahue.com/">this</a>
          </font></p></td></tr>
      </tbody></table>
      
        </td></tr>
      </tbody>  
  </table>
</body>

</html>
